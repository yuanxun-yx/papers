<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Input-side Traps for Governing Undisclosed LLM Reviewing</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="Xun Yuan">
</head>
<body>
  <h1>Input-side Traps for Governing Undisclosed LLM Reviewing</h1>

  <p><strong>Author:</strong> Xun Yuan</p>

  <p>
    <strong>PDF:</strong>
    <a href="manuscript.pdf">Download</a>
  </p>

  <h2>Abstract</h2>
  <p>
    Peer review at major AI venues is increasingly affected by low-quality, LLM-generated reviews, 
    while existing output-side measures lack enforceability or cannot distinguish LLM substitution 
    from benign assistance. This paper elevates what is often an individual, illicit use of input-side 
    injection into a venue-controlled, adversarially structured governance framework. It also introduces 
    a PDF-based trap architecture that embeds adversarially robust signals to preserve a structural 
    cost asymmetry against undisclosed LLM misuse.
  </p>

  <h2>BibTeX</h2>
  <pre><code>@misc{yuan_input-side_2025,
  author = {Yuan, Xun},
  title  = {Input-side Traps for Governing Undisclosed LLM Reviewing},
  year   = {2025},
  doi    = {10.5281/zenodo.17702164},
  url    = {https://zenodo.org/records/17702164}
}</code></pre>
</body>
</html>